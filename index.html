<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<!-- saved from url=(0030)https://remimz.github.io/#home -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" class="eye-protector-processed" style="background-color: rgb(255, 255, 255);"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!--<link rel="icon" href="https://hegsns.github.io/Rice_logo.jpg" type="image/x-icon">-->
<!--<link rel="shortcut icon" href="https://hegsns.github.io/Rice_logo.jpg" type="image/x-icon">-->
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/">

<link rel="stylesheet" href="./Guanchu Wang_files/jemdoc.css" type="text/css">
<title>Guanchu Wang</title>
</head>
<body class="eye-protector-processed" style="background-color: rgb(255, 255, 255); transition: background-color 0.3s ease 0s;">

<div class="menu"> 
<a href="https://guanchuwang.github.io/home/">Home</a>
<!-- <a href="#education">Education</a> -->
<!-- <a href="#work">Working</a> -->
<a href="https://guanchuwang.github.io/home/pub.html">Publications</a>
<a href="https://guanchuwang.github.io/home/demo.html">Research Demo</a>
<a href="https://guanchuwang.github.io/home/teach.html">Teaching and Mentoring</a>
<!-- <a href="#project">Project</a>  -->
<!-- <a href="#services">Academic</a>  -->
<!-- <a href="#people">People</a>  -->
<!-- <a href="#awards">Awards</a> -->
</div>

 
<div id="home" class="anchor"></div>
<div id="container">
<div class="container eye-protector-processed" style="transition: background-color 0.3s ease 0s; background-color: rgb(255, 255, 255);">
<div id="toptitle" class="eye-protector-processed" style="border-bottom-color: rgba(0, 0, 0, 0.35);">
<h1>Guanchu (Gary) Wang <font family="FangSong"></font></h1>
</div>

<table class="imgtable"><tbody><tr>
<td align="left" width="750px">
<p>Ph.D. Candidate<br>
  Department of Computer Science<br>
  Rice University <br>
  <b>Address:</b> Duncan Hall 3014, 6100 Main St, Houston, TX 77005 <br>
  <b>Email:</b> Guanchu.Wang@rice.edu <br>
  <a href="./Guanchu Wang_files/Guanchu_CV.pdf"><font color="#004469">[CV]</font></a>
  <a href="https://www.linkedin.com/in/guanchu-wang-733785111/"><font color="#004469">[Linkedin]</font></a>
  <a href="https://scholar.google.com/citations?user=_QL5218AAAAJ&hl=en"><font color="#004469">[Google Scholar]</font></a>
  <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Guanchu"><font color="#004469">[DBLP]</font></a>
  <a href="https://www.researchgate.net/scientific-contributions/2137741000_Guanchu_Wang"><font color="#004469">[ResearchGate]</font></a>
  <a href="https://paperswithcode.com/search?q=author%3AGuanchu+Wang"><font color="#004469">[Paper with code]</font></a>
</p>
</td>
<td align="left"><img src="./Guanchu Wang_files/photo.jpeg" alt="left" width="250px">&nbsp;</td>
</tr></tbody></table>

<!-- 	Before joining Rice University, I was working as a research assistant in Texas A&amp;M University (<a href="http://faculty.cs.tamu.edu/xiahu/">Prof. Xia Hu</a> group) in 2020 and Westlake university (Professor <a href="https://milab.westlake.edu.cn/index.html">Donglin Wang</a> group) in 2019.
	From 2012 to 2019, I recevied B.S. degree from Dalian University of Technology (DUT) 
	and M.S. degree from University of Science and Technology of China (USTC), where I was fortunate for having been supervised by Prof. <a href="https://dblp.org/pers/hd/g/Gong_0001:Chen">Chen Gong</a> and <a href="https://scholar.google.com/citations?user=SwJdUkwAAAAJ&hl=zh-CN">Zhenyuan Xu</a>.
</p>
<p> -->

<h2>About me</h2>

    <p>I am an assistant professor in the Department of Computer Science at UNC Charlotte. I earned my Ph.D. in Computer Science from Rice University under the guidance of Dr. <a href="https://cs.rice.edu/~xh37/index.html">Xia Hu</a> and <a href="https://cs.rice.edu/~vb21/"> Vladimir Braverman</a>.
My primary research focuses on the <font color="#004469">efficiency</font> and <font color="#004469">safety</font> of large language models. In addition, I collaborate closely with health informaticians at Baylor College of Medicine, leveraging AI to address critical challenges in <font color="#004469">healthcare</font>. 
    </p>
	
<!--     <p>I am currently a PhD. candidate in the department of computer science at Rice University supervised by Professors <a href="https://cs.rice.edu/~xh37/index.html">Xia Hu</a> and <a href="https://cs.rice.edu/~vb21/"> Vladimir Braverman</a>.
My primary research focus is in the realm of <font color="#004469">Trustworthy AI</font>, a critical area that demands the infusion of trust throughout the AI lifecycle.
Within this overarching theme, my research focuses on <font color="#004469">explainable AI (XAI)</font>, developing algorithms to faithfully explain the behaviors of AI models and systems.
My research can be widely applied to most foundation models, including multilayer perception, graph neural networks, convolutional neural networks, vision transformers, and <font color="#004469">large language models</font>.
    </p> -->
<!-- <p>
In addition to my primary focus, I collaborate closely with health informaticians at Baylor College of Medicine, leveraging AI to address critical challenges in <font color="#004469">healthcare</font>. 
<p> -->
	
<p>
<b><font color="red">I'm currently on the job market seeking for academic opportunities.</font></b>	
</p>    
	
<div id="education"><h2>Education</h2></div>

<ul>
<li><b>From 2021.9 to now</b>: Ph.D. candidate, Computer Science, Rice University.
</li>	

<div id="ustc_edu" style="cursor:pointer;color:black" onclick="document.all.ustc_edu_ch1.style.display=(document.all.ustc_edu_ch1.style.display ==&#39;none&#39;)?&#39;&#39;:&#39;none&#39;">	
<li><b>From 2016.9 to 2019.6</b>: M.S., Information Science and Technology, University of Science and Technology of China (USTC). 
</li></div>
<div id="ustc_edu_ch1" style="display:none">
<b>‚Ä¢&nbsp; GPA</b>: 3.5/4.3 <a href="https://hegsns.github.io/Master_GPA.pdf"><font color="#004469">[Details]</font></a>.
<br><b>‚Ä¢&nbsp; TOEFL</b>: 99/120 (Listening:25/30 Reading:26/30 Writing:27/30 Speaking:21/30) <a href="https://hegsns.github.io/TOEFL.pdf"><font color="#004469">[Details]</font></a>.
<br><b>‚Ä¢&nbsp; GRE</b>: 323 + 3.5 (Verbal:153 Quantitative:170 Writing:3.5) <a href="https://hegsns.github.io/GREscore.pdf"><font color="#004469">[Details]</font></a>.<br>

<br>
</div>

<div id="dlut_edu" style="cursor:pointer;color:black" onclick="document.all.dlut_edu_ch1.style.display=(document.all.dlut_edu_ch1.style.display ==&#39;none&#39;)?&#39;&#39;:&#39;none&#39;">	
<li><b>From 2012.9 to 2016.6</b>: B.S., Information and Communication Engineering, Dalian University of Technology (DUT).
</li></div>
<div id="dlut_edu_ch1" style="display:none">
<b>‚Ä¢&nbsp; GPA</b>: 3.8/4.0 (TOP 3%) <a href="https://hegsns.github.io/Bachelor_GPA.pdf"><font color="#004469">[Details]</font></a>.<br>
<b>‚Ä¢&nbsp; GPA</b>: Outstanding undergraduate thesis award (Top 2%).<br>
</div>


</ul>

<div id="work"><h2>Research Experience</h2></div>

<ul>
<!--
<li> <b>From 2021.9 to now</b>: Graduate research assistant, Rice University, USA.
<br>	
</li><li> <b>From 2020.9 to 2021.8</b>: Graduate research assistant, Texas A&amp;M University, USA.
</li>
<br>
-->
<li> <b>From 2024.5 to 2024.8</b>: Research intern, VISA Research, Foster City, CA, USA.
<br> 	
</li><li> <b>From 2020.9 to 2021.8</b>: Graduate research assistant, Texas A&amp;M University, USA.
<br> 	
</li><li> <b>From 2019.9 to 2020.1</b>: Research assistant, Westlake University, Hangzhou, P.R.C.
<br> 
</li>

</ul>


<h2>What's New</h2> 
	
<ul>
<li> <b>05/2025:</b> Our paper <i>MAIN-RAG: Multi-Agent Filtering Retrieval-Augmented Generation.</i> is accepted by ACL 2025! Credits to all co-authors! See <a href="https://arxiv.org/pdf/2501.00332"><font color="#004469">[Paper]</font></a>.
</li>
<li> <b>10/2024:</b> Our paper <i>Taylor Unswift: Secured Weight Release for Large Language Models via Taylor Expansion.</i> has attracted <font color="red"><b>175.9K</b></font> views and <font color="red"><b>1.7K</b></font> like on Twitter; <font color="red"><b>1K</b></font> like on Xiaohongshu; and <font color="red"><b>5.6K</b></font> impressions and <font color="red"><b>70</b></font> reactions on LinkedInüî•! See <a href="https://x.com/yuchenj_uw/status/1851308185153622138?s=46"><font color="#004469">[Twitter]</font></a> <a href="https://www.xiaohongshu.com/explore/671915c9000000001b0123c6?app_platform=ios&app_version=8.58.1&share_from_user_hidden=true&xsec_source=app_share&type=normal&xsec_token=CBKsf2Q9k_skAHaUlRxsorE88s5bu5ihh3VbVwhzMSHrU=&author_share=1&xhsshare=WeixinSession&shareRedId=N0o4QkU3NU02NzUyOTgwNjY0OTc3PEc_&apptime=1730392850&share_id=f5fcd536b93546818030283865b862e0&exSource=."><font color="#004469">[Xiaohongshu]</font></a> <a href="https://www.linkedin.com/feed/update/urn:li:activity:7250899298538672128/"><font color="#004469">[LinkedIn]</font></a>
</li>
<li> <b>09/2024:</b> Our work <i>Taylor Unswift: Secured Weight Release for Large Language Models via Taylor Expansion.</i> is accepted by EMNLP 2024! Credits to all co-authors! See <a href="https://www.researchgate.net/publication/381662847_Secured_Weight_Release_for_Large_Language_Models_via_Taylor_Expansion"><font color="#004469">[Paper]</font></a>.
</li>	
<li> <b>09/2024:</b> Our benchmark paper <i>KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches.</i> is accepted by EMNLP 2024 Findings! Credits to all co-authors! See <a href="https://arxiv.org/abs/2407.01527"><font color="#004469">[Paper]</font></a>.
</li>
<li> <b>08/2024:</b> Our open-source dataset, corpora, and benchmark paper on LLMs for rare disease diagnosis have been released! See <a href="https://huggingface.co/datasets/guan-wang/ReDis-QA"><font color="#004469">[Dataset]</font></a> <a href="https://huggingface.co/datasets/guan-wang/ReCOP"><font color="#004469">[Corpora]</font></a> <a href="https://github.com/guanchuwang/redis-bench?tab=readme-ov-file"><font color="#004469">[Code]</font></a> <a href="https://arxiv.org/abs/2408.08422"><font color="#004469">[Paper]</font></a>.
</li>
<li> <b>07/2024:</b> Our work <i>Efficient GNN Explanation via Learning Removal-based Attribution</i> is published on TKDD! See <a href="https://arxiv.org/abs/2306.05760"><font color="#004469">[Paper]</font></a> and <a href=""><font color="#004469">[Code]</font></a>.
</li>
<li> <b>07/2024:</b> Our benchmark paper on KV Cache Compression has been released! See <a href="https://arxiv.org/abs/2407.01527"><font color="#004469">[Paper]</font></a> and <a href="https://github.com/henryzhongsc/longctx_bench"><font color="#004469">[Code]</font></a>.
</li>
<li> <b>06/2024:</b> Our benchmark paper on training foundational time-series models has been released! See <a href="https://arxiv.org/abs/2402.04678v2"><font color="#004469">[Paper]</font></a>, <a href="https://github.com/daochenzha/ltsm"><font color="#004469">[Code]</font></a>, <a href=""><font color="#004469">[Twitter]</font></a>, <a href="https://towardsdatascience.com/time-series-are-not-that-different-for-llms-56435dc7d2b1?gi=3ec22ff427ce&source=login--------------------------global_nav-----------"><font color="#004469">[Medium]</font></a>, and <a href="https://x.com/yunengchuang/status/1806032177554919862?s=46"><font color="#004469">[Zhihu]</font></a>.
</li>
<li> <b>05/2024:</b> Our work <i>TVE: Learning Meta-attribution for Transferable Vision Explainer</i> is accepted by ICML 2024! See <a href="https://www.researchgate.net/publication/377926010_TVE_Learning_Meta-attribution_for_Transferable_Vision_Explainer"><font color="#004469">[Paper]</font></a>.
</li>
<li> <b>03/2024:</b> My pleasure of receiving the <font color="red"><b>Doctoral Forum Travel Award</b></font> in SDM 2024 (Society for Industrial and Applied Mathematics)! 
</li>
<li> <b>02/2024:</b> It is my honor to have received the admission of <a href="https://midas.umich.edu/future-leaders-summit-2024/"><font color="red"><b>Future Leader Summit 2024</b></font></a>! 
</li>
<li> <b>11/2023:</b> Our <i>Natural Language UI</i> for the NORD Rare-disease Database is online! <a href="https://hegsns-rare-disease-gpt.hf.space"><font color="#004469">[APP]</font></a>. 
</li>
<li> <b>10/2023:</b> Our work <i>DiscoverPath: A Knowledge Refinement and Retrieval System for Interdisciplinarity on Biomedical Research</i> achieves the <font color="red"><b>Best Paper Honorable Mention</b></font> in CIKM 2023! Thanks to all of the co-authors! See <a href="https://www.researchgate.net/publication/369755614_DiscoverPath_A_Knowledge_Refinement_and_Retrieval_System_for_Interdisciplinarity_on_Biomedical_Research"><font color="#004469">[Paper]</font></a>. 
</li>
<li> <b>09/2023:</b> Our work <i>Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model</i> is accepted by NeurIPS 2023! See [<a href="https://arxiv.org/abs/2305.15265">Paper</a>]. <br>
</li>
<li> <b>09/2023:</b> Our work <i>Chasing Fairness under Distribution Shift: a Model Weight Perturbation Approach</i> is accepted by NeurIPS 2023! See [<a href="https://openreview.net/forum?id=DVjyq5eCAD&referrer=%5Bthe%20profile%20of%20Guanchu%20Wang%5D(%2Fprofile%3Fid%3D~Guanchu_Wang1)">Paper</a>]. <br>
</li>
<li> <b>08/2023:</b> Our work <i>DiscoverPath: A Knowledge Refinement and Retrieval System for Interdisciplinarity on Biomedical Research</i> is accepted by CIKM 2023 demo track! See [<a href="https://www.researchgate.net/publication/369755614_DiscoverPath_A_Knowledge_Refinement_and_Retrieval_System_for_Interdisciplinarity_on_Biomedical_Research">Paper</a>] [<a href="https://github.com/ynchuang/ad2kg">Code</a>] [<a href="https://reurl.cc/nDDeq1">Demo</a>] [<a href="http://www.discoverpath.top/">FrontEnd</a>]. <br>
</li>
<li> <b>06/2023:</b> Our work <i>Mitigating Algorithmic Bias with Limited Annotations</i> is accepted by ECML-PKDD 2023! See <a href="https://arxiv.org/pdf/2207.10018.pdf"><font color="#004469">[Paper]</font></a>.
</li>
<li> <b>04/2023:</b> Our work <i>DIVISION: Memory Efficient Training via Dual Activation Precision</i> is accepted by ICML 2023! See <a href="https://openreview.net/forum?id=Bxfp0zWygq&referrer=%5Bthe%20profile%20of%20Guanchu%20Wang%5D(%2Fprofile%3Fid%3D~Guanchu_Wang1)"><font color="#004469">[Paper]</font></a>.
</li>
<li> <b>03/2023:</b>Invited poster in the Energy High Performance Computing Conference sponsored by the Ken Kennedy Institute.
</li>
<li> <b>03/2023:</b>Invited talk in the seminor of CS department at Rice University. See <a href="https://www.dropbox.com/s/05r1ra8rqxisdu3/COMP600_seminor_SHEAR.mp4?dl=0"><font color="#004469">[Talk]</font></a>.
</li>
<li> <b>02/2023:</b> Our efficient XAI survey paper has been reported by TechBeat and Tencent Developer! See <a href="https://www.techbeat.net/article-info?id=4646"><font color="#004469">[TechBeat]</font></a> and <a href="https://cloud.tencent.com/developer/article/2229319?areaSource=&traceId="><font color="#004469">[Tencent]</font></a>.
</li>
<li> <b>02/2023:</b> Our opensource project <i>DiscoverPath</i> has been released! See <a href="http://www.discoverpath.top/"><font color="#004469">[DiscoverPath]</font></a> for online demo; and <a href="https://github.com/ynchuang/ad2kg"><font color="#004469">[Github]</font></a> for source code.
</li>
<li> <b>01/2023:</b> Our work <i>CoRTX: Contrastive Learning for Real-time Explanations.</i> is accepted by ICLR 2023! See <a href="https://openreview.net/forum?id=L2MUOUp0beo"><font color="#004469">[Paper]</font></a>
</li>	
<li> <b>12/2022:</b> It's my honor to have received the <a href="https://kenkennedy.rice.edu/funding/memorial-honorary-fellowships"><font color="red"><b>KKI 2022/23 Graduate Fellowship</b></font></a> from the <a href="https://kenkennedy.rice.edu/">Ken Kennedy Institute</a> of Rice University. See <a href="https://kenkennedy.rice.edu/news/current-news/ken-kennedy-institute-graduate-fellowship-program-awards-65000-nine-fellows"><font color="#004469">[the Full List]</font></a>. 
</li>
<li> <b>10/2022:</b> Our real-time object detection demo work achieves the <font color="red"><b>Best Paper Award</b></font> in CIKM 2022! Thanks to all of the co-authors! See <a href="https://arxiv.org/abs/2202.07503"><font color="#004469">[Paper]</font></a>. 
</li>
<li> <b>09/2022:</b> Our <i>Accelerating Shapley Explanation via Contributive Cooperator Selection</i> is selected as the <font color="red"><b>Spotlight</b></font> of ICML 2022! See <a href="https://icml.cc/virtual/2022/spotlight/16674"><font color="#004469">[Paper]</font></a>.
</li>
<li> <b>08/2022:</b> Tutorial of medical anomaly detection using TODS has been released! See <a href="https://github.com/ryanbeckwith/medical_anomaly_detection"><font color="#004469">[Github]</font></a>.
</li>
<li> <b>08/2022:</b> Our real-time object detection system based on MAX78000 is accepted by CIKM 2022 Demo Track! See <a href="https://github.com/datamllab/BED_main"><font color="#004469">[Github]</font></a>.
</li>
<li> <b>06/2022:</b> Our work <i>Accelerating Shapley Explanation via Contributive Cooperator Selection</i> is accepted by ICML 2022! See <a href="https://arxiv.org/pdf/2206.08529.pdf"><font color="#004469">[Paper]</font></a>.
</li>
<li> <b>12/2021:</b> Our real-time object detection based on MAX78000 for edge devices has been released! See <a href="https://github.com/datamllab/BED_main"><font color="#004469">[Github]</font></a>.
</li>
<li> <b>10/2021:</b> Two papers <i>Fairness via Representation Neutralization</i> and <i>Revisiting Time Series Outlier Detection: Definitions and Benchmarks</i> are accepted by NeurIPS 2021!
</li>
<li> <b>09/2020:</b> Our opensource package <i>TODS</i> is accepted by AAAI 2021 Demo Track! See <a href="https://arxiv.org/abs/2009.09822"><font color="#004469">[Paper]</font></a>.
</li>
<li> <b>06/2020:</b> Opensource package <i>TODS</i> for end-to-end time-series outlier detection has been online! See <a href="https://github.com/datamllab/tods"><font color="#004469">[Github]</font></a>.
</li>
<li> <b>04/2020:</b> Our work <i>Independent Skill Transfer for Deep Reinforcement Learning</i> is accepted by IJCAI 2020 (Accepted rate 12.6%)! See <a href="https://www.ijcai.org/proceedings/2020/0401.pdf"><font color="#004469">[Paper]</font></a>.
</li>
</ul>
	
<!--
<div style="height:200px;width:1150px;overflow:auto;background:#EEEEEE;"> </div>
-->
	
	

<!-- <div id="previous_research_publication" style="cursor:pointer;color:black" onclick="document.all.previous_research_publication_ch1.style.display=(document.all.previous_research_publication_ch1.style.display ==&#39;none&#39;)?&#39;&#39;:&#39;none&#39;">	
<b>Publications related to signal processing(Click for details)</b>
</div>
<div id="previous_research_publication_ch1" style="display:none">
</div> -->





<!--
	
<div id="project"><h2>Projects</h2></div>
<ul>
</li>
<li>Efficiency of Large Language Models, 2023.2-now
</li>
<li>Interpretable Recommder System, 2021.10-2022.12
</li>
<li>AI Fairness, 2021.2-2022.2.
</li>
<li>Object detection on edge devices, 2021.2-2021.10
</li>
<li>TODS: Time-series outlier detection, 2020.5-2021.2.
</li>
<li>Deep reinforcement learning for robotic controlling, 2019.10-2020.1
</li>
<li>Theory of optical wireless communication and network, 2016.9-2018.4
</li>
</ul>


<div id="services"><h2>Academic</h2></div>
<ul>
<li> Poster at Rice University PhD. Recuiting, Houston, US., 2022. </li>	
<li> Unofficial NeurIPS Workshop at Rice University, Houston, US., 2021. </li>	
<li> Vision And Learning Seminar (VALSE), Hefei, Anhui, China, 2019. </li>	
<li> ShanghaiTech Workshop on Information, Learning and Decision, Shanghai, China, 2018. </li>
<li> Visiting student, Chinese Academy of science, Beijing, China, 2015. </li>
</ul>


	
<div id="people"><h2>People</h2></div>
	
<div style="height:150px;width:950px;overflow:auto;background:#EEEEEE;">
<ul>
<li> 2023-now. Huiyuan Chen. VISA Inc.
</li>
<li> 2023-now. Zhaozhuo Xu. assistant Professor in Stevens Institute of Technology.
</li>
<li> 2022-2023. Yao Rong. University of T√ºbingen.
</li>
<li> 2021-now. Yu-Neng Chuang. Rice University.
</li>
<li> 2021-now. Fan Yang. Rice University.
</li>
<li> 2021-now. Ninghao Liu. University of Georgia.
</li>
<li> 2021-now. Mengnan Du. New Jersey Institute of Technology.
</li>
<li> 2020. Yue Zhao. Leadership of opensources Pyod, TOD, SUOD and PyGOD (6.1k+ star, 1.2k+ fork in total).
</li>
<li> 2020-now. Daochen Zha. Leadership of opensources RLcard, DouZero and AutoVideo (4.8k+ star, 800+ fork in total).
</li>
<li> 2020-now. Kwei-Herng Lai. Leadership of opensource TODS (500+ star, 100+ fork).
</li>
<li> 2020-now. Xia Hu. (my PhD supervisor) Rice University. One of authors of the Neural Collaborative Filtering algorithm. Leadership of AutoKeras, RLcard, TODS (10k+ star, 1.8k fork in total). 
</li>
<li> 2019. JinXin Liu. Westlake University.
</li>
<li> 2019. Donglin Wang. Westlake University. ÂõΩÂÆ∂ÁßëÊäÄÂàõÊñ∞2030ÈáçÂ§ßÈ°πÁõÆÈ¶ñÂ∏≠ÁßëÂ≠¶ÂÆ∂.
</li>
<li> 2016-now. Zhimeng Jiang. Texas A&M University.
</li>
<li> 2016-now. Kaixiong Zou. Rice Univeristy.
</li>
<li> 2016-2019. Difan Zou. The University of Hong Kong.
</li>
<li> 2016-2019. Chen Gong. (my MS supervisor) University of Science and Technology of China. ÂõΩÂÆ∂ÈùíÂπ¥ÂçÉ‰∫∫. 2016Âπ¥Ê±ÇÊòØÊù∞Âá∫ÁßëÂ≠¶ÂÆ∂Â•ñËé∑ÂæóËÄÖ.
</li>
<li> 2012-2016. Xiaonan Liu. King's College London.
</li>
</ul>
</div>

-->
	
<!--

<li> Selected invited Review: NeurIPS, KDD, AAAI, SDM, CIKM, SIGIR, INFORM, IEEE Trans. and Journal. </li>

<h2>Honors and Awards</h2>
<ol>
<li><b>Major Awards</b>, USTC, 2016, 2017, 2018.</li> 
<li><b>Outstanding Graduate (top 4%)</b>, <a href="https://en.wikipedia.org/wiki/Dalian">Dalian</a>, 2016.</li> 
<li><b>Outstanding Bachelor Thesis (top 5%)</b>, DUT, 2016.</li> 
<li><b>National Second Prize</b>, China National Mathematical Modeling Contest, 2016.</li>
<li><b>Honorable Mention</b>, Mathematical Contest in Modeling (MCM), 2016.</li>
<li><b>Scholarship of Academic and Innovation</b>, DUT, 2013, 2014, 2015.</li>
<li><b>Scholarship of <a href="https://www.sumitomocorp.com/ja/jp/about/company/profile"> Sumitomo Corporation</a></b>, DUT, 2013.</li> 
</ol>
	
<h2>Key Skill</h2>
<ol>
<li>Python, Matlab, C++, Verilog.</li>
<li>Numerical optimization, system simulation, and statistics.</li>
<li>Python/Pytorch, Labview/USRP, Verilog/ISE/FPGA.</li>
<li>Cooperation (Github online co-working) & teamwork & paper writing.</li>
</ol>
-->

<!--
<h2>Co-worker</h2>
&nbsp;&nbsp; Xiaonan Liu (King's College London), 
	Chen Gong (USTC), Xiaodong Wang (Columbia University), Donglin Wang (Westlake University),
	Xuanting Cai (Meta Facebook)
-->

<!--
<h2>Hobbies</h2>
<p>Badminton, Painting, Reading.</p>


<h2>Views</h2>

<a href="https://clustrmaps.com/site/1b1jj" title="Visit tracker"><img src="./Guanchu Wang_files/map_v2.png"></a>

-->
	
</font>

<!--

<h2>Research Experiences</h2>
<ol>

<li><b>DATA Lab</b> in Texas A&amp;M Universit, working promotely, 2020.2 - present.</li>
<p><b>‚Ä¢&nbsp; Anomaly Detection.</b></p>
<table class="imgtable"><tbody><tr>
<td><img src="./Guanchu Wang_files/ad.jpg" alt="left" width="240px">&nbsp;</td>
<td align="left"> Update...
</td>
</tr></tbody></table>
<br>

<div id="westlake" style="cursor:pointer;color:black" onclick="document.all.westlake_ch.style.display=(document.all.westlake_ch.style.display ==&#39;none&#39;)?&#39;&#39;:&#39;none&#39;">
<li><b>Machine Intellignece Lab</b> in Westlake University, Hangzhou, P.R.C., Research assistant, 2019.10 - 2020.1.</li></div>

<div id="westlake_ch" style="display:none">
<p><b>‚Ä¢&nbsp; Deep Reinforcement learning.</b></p>
<table class="imgtable"><tbody><tr>
<td><img src="./Guanchu Wang_files/HCA.gif" alt="left" width="240px">&nbsp;
<br><img src="./Guanchu Wang_files/HCU.gif" alt="left" width="240px">&nbsp;</td>
<td align="left">We use deep reinforcement learning to design efficient algorithms for skill transfer and transition.
In skill transfer, the agent learns task-independent primitive skills in source environments, and re-utilize the skills in target environemnts;
while in skill transtion, the agent manage to transition between multiple skills in order to accomplish specific complex task.
Both skill transfer and transtion can greatly enhance the learning efficiency and generalization ability in comparison to conventional RL algorithms.

</table>
<br>
</div>

<div id="magcharging" style="cursor:pointer;color:black" onclick="document.all.magcharging_ch.style.display=(document.all.magcharging_ch.style.display ==&#39;none&#39;)?&#39;&#39;:&#39;none&#39;">
<li><b>Magcharging Inc.</b>, Shenzhen, P.R.C., Intern, 2019.7 - 2019.9.</li></div>
<div id="magcharging_ch" style="display:none">
<p><b>‚Ä¢&nbsp; Ratio frequency wirless charging.</b></p>
<table class="imgtable"><tbody><tr>
<td><img src="./Guanchu Wang_files/RFC.png" alt="left" width="240px">&nbsp;</td>
<td align="left">
We realize a prototype machine of ratio frequency wireless charging system.
In transmitter-side, two USRPs are utilized to control three directional antennas for energy emission;
in receiver-side, an intergrated device including an RF harvester and an RF-DC convertor is exploited
for energy havesting.
We also build a backscattering link for real-time optimization of transmitted signal
based on blind adaptive beamforming algorithm.
The receiver-side power is up to 12mW at the distance of 2m and 6mW at 3m.
Specifically, it includes the techniques:
<br>
&nbsp;&nbsp; - MIMO, blind adaptive beamforming.
<br>
&nbsp;&nbsp; - Backscattering communication.
<br>
Here is our real-time video: <b><a href="https://hegsns.github.io/3nt2m.mp4">Triple-antenna 2m</a></b>, <b><a href="https://hegsns.github.io/3nt3m.mp4">Triple-antenna 3m</a></b>.
</td>
</tr></tbody></table>
<br>
</div>

<div id="ustc_owc" style="cursor:pointer;color:black" onclick="document.all.ustc_owc_ch1.style.display=(document.all.ustc_owc_ch1.style.display ==&#39;none&#39;)?&#39;&#39;:&#39;none&#39;">
<li><b> Optical Wireless Communication and Network Center </b> in USTC, Anhui, P.R.C., Postgraduate Student, 2016.9 - 2019.6.
</li></div>
<div id="ustc_owc_ch1" style="display:none">
<p><b>‚Ä¢&nbsp; Optical network system:</b></p>
<table class="imgtable"><tbody><tr>
<td><img src="./Guanchu Wang_files/OWLAN.jpg" alt="left" width="240px">&nbsp;</td>
<td align="left">
The conventional media access control of wireless local area network (WLAN) can hardly support
the transmission of big-data in the future,
and the upcoming optical network tends to be accessed in a multipoint pattern for big-data transmission.
We devise a doub-station access protocol based on CSMA/CA (DS-CSMA/CA) for optical wireless local area
network (OWLAN), and propose the the algorithm to numerically maximize the throughput versus initial
contention window and partner distribution map.

</td>
</tr></tbody></table>

<p><b>‚Ä¢&nbsp; Multiple access system:</b></p>
<table class="imgtable"><tbody><tr>
<td><img src="./Guanchu Wang_files/turbo_processing1_total.png" alt="left" width="250px">&nbsp;</td>
<td align="left">
We study the physical-layer characteristics of multiple access for weak signal optical communication including
multi-user and multi-layer communications, where the scenairo of multi-user communication is single transmiter to multiple receivers
and multiple transmitters to mulitple receivers;
and that of multi-layer communication is to simutaneously transmit several types of serives with different rates.

</td>
</tr></tbody></table>


Furthermore, it includes the techneques:
<br>
&nbsp;&nbsp; - Numerical optimization of multiple access achievable rates.
<br>
&nbsp;&nbsp; - Multi-user estimation, signal processing, correcting code.
<br>
&nbsp;&nbsp; - Hidden Markov Model.


<p><b>‚Ä¢&nbsp; Weak signal optical communication:</b></p>
<table class="imgtable"><tbody><tr>
<td><img src="./Guanchu Wang_files/frame_structure.png" alt="left" width="150px">&nbsp;</td>
<td align="left">
We set up a real-time weak signal optical communication system via photon counting, digitial signal processing and error correcting encoding&amp;decoding based on FPGA.
The throughput is up to 1Mbps over 1km non-line of sight transmission link (most advanced over the world at that time).

</td>
</tr></tbody></table>
</div>


<br>
&nbsp;&nbsp; - Photon conunting technology.
<br>
&nbsp;&nbsp; - Digital signal processing (synchronization, estimation &amp; symbol detection).
<br>
&nbsp;&nbsp; - Error correcting code (convolutional, RS, LDPC code).

</ol>

-->

<div class="jvectormap-tip"></div>

</div>
</div>
</body></html>
